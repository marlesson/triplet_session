{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Embedding\n",
    "\n",
    "https://fridayexperiment.com/how-to-encode-time-property-in-recurrent-neutral-networks/\n",
    "\n",
    "https://arxiv.org/pdf/1708.00065.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_embedding_size = 10\n",
    "output_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10]), torch.Size([10]), torch.Size([10, 5]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_weight = Variable(torch.randn(1, hidden_embedding_size), requires_grad=True)\n",
    "emb_bias   = Variable(torch.randn(hidden_embedding_size), requires_grad=True)\n",
    "emb_time   = Variable(torch.randn(hidden_embedding_size, output_dim), requires_grad=True)\n",
    "\n",
    "emb_weight.shape, emb_bias.shape, emb_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(x):\n",
    "    x = torch.softmax(x * emb_weight + emb_bias, dim=1)\n",
    "    x = torch.matmul(x, emb_time)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([1, 2]).unsqueeze(1)\n",
    "\n",
    "x = call(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_embedding_size, output_dim):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "        self.emb_weight = nn.Parameter(torch.randn(1, hidden_embedding_size)) # (1, H)\n",
    "        self.emb_bias = nn.Parameter(torch.randn(hidden_embedding_size)) # (H)\n",
    "        self.emb_time = nn.Parameter(torch.randn(hidden_embedding_size, output_dim)) # (H, E)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input (B, W, 1)\n",
    "        x = torch.softmax(input * self.emb_weight + self.emb_bias, dim=2) # (B, W, H)\n",
    "        x = torch.matmul(x, self.emb_time) # (B, W, E)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n",
      "torch.Size([2, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "te = TimeEmbedding(50, 10)\n",
    "x = torch.Tensor([[1, 2, 3], [3,4,5]]).unsqueeze(2)\n",
    "\n",
    "r = te(x)\n",
    "print(x.shape)\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1]), torch.Size([1, 10]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, emb_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2, 3], \n",
    "                  [3, 4, 5]]).unsqueeze(2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = (x * emb_weight + emb_bias)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.softmax(c, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(c, emb_time).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2240, -0.7491, -1.0287,  0.4589,  0.1873],\n",
       "        [-1.1723, -0.2210, -0.7488,  0.5377,  1.5246],\n",
       "        [ 1.9532,  1.2989, -0.5827,  0.3105, -1.3928],\n",
       "        [ 0.1894,  0.3690,  0.5805,  0.0558, -1.6729],\n",
       "        [-1.6623,  0.6312,  0.0351,  0.3948,  1.3067],\n",
       "        [-1.9284, -0.4763,  0.3184, -1.0626,  0.5354],\n",
       "        [ 0.1490,  1.0069,  1.1961, -0.2049, -1.4269],\n",
       "        [-1.0915,  1.2201, -0.5009,  0.7608, -0.8282],\n",
       "        [-0.4441, -1.7258, -1.5327,  0.1768, -1.3967],\n",
       "        [-0.5657,  0.8329,  1.1479,  1.8829, -0.8780]], requires_grad=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('triplet-session': conda)",
   "language": "python",
   "name": "python361264bittripletsessioncondad5cbd839d71f4524a2e1ac2238b70b72"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
