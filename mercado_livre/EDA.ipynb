{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, udf, size\n",
    "from pyspark.sql.functions import collect_set, collect_list, lit, sum, udf, concat_ws, col, count, abs, date_format, \\\n",
    "    from_utc_timestamp, expr, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|item_bought|        user_history|\n",
      "+-----------+--------------------+\n",
      "|    1748830|[[1786148, 2019-1...|\n",
      "|     228737|[[643652, 2019-10...|\n",
      "+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('/media/backup/datasets/mercado_livre/train_dataset.jl')\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+-------+----------+----------+--------------------+\n",
      "|category_id|condition|           domain_id|item_id|     price|product_id|               title|\n",
      "+-----------+---------+--------------------+-------+----------+----------+--------------------+\n",
      "|  MLM170527|      new|MLM-INDIVIDUAL_HO...| 111260|1150000.00|      null|Casa Sola En Vent...|\n",
      "|  MLM151595|      new|     MLM-VIDEO_GAMES| 871377|   1392.83|  15270800|Resident Evil Ori...|\n",
      "+-----------+---------+--------------------+-------+----------+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meta = spark.read.json('/media/backup/datasets/mercado_livre/item_data.jl')\n",
    "df_meta.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_bought=1748830, user_history=[Row(event_info='1786148', event_timestamp='2019-10-19T11:25:42.444-0400', event_type='view'), Row(event_info='1786148', event_timestamp='2019-10-19T11:25:57.487-0400', event_type='view'), Row(event_info='RELOGIO SMARTWATCH', event_timestamp='2019-10-19T11:26:07.063-0400', event_type='search'), Row(event_info='1615991', event_timestamp='2019-10-19T11:27:26.879-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-19T11:28:36.558-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-19T11:28:40.827-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-19T11:30:42.089-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-19T21:51:29.622-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-19T21:52:09.281-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-19T21:52:41.863-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-19T21:54:16.119-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-19T21:54:40.629-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-19T21:54:57.329-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-19T22:00:04.577-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-20T10:36:47.525-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-20T10:37:23.202-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-20T10:37:47.699-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-20T19:28:14.619-0400', event_type='view'), Row(event_info='1615991', event_timestamp='2019-10-20T19:28:41.646-0400', event_type='view')])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"session_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|item_bought|        user_history|session_id|\n",
      "+-----------+--------------------+----------+\n",
      "|    1748830|[[1786148, 2019-1...|         0|\n",
      "|     228737|[[643652, 2019-10...|         1|\n",
      "|    1909110|[[248595, 2019-10...|         2|\n",
      "|    1197370|[[RADIOBOSS, 2019...|         3|\n",
      "|    2049207|[[AMAZFIT BIP, 20...|         4|\n",
      "|    1046119|[[TAMPA TRASEIRA ...|         5|\n",
      "|    1206282|[[400631, 2019-10...|         6|\n",
      "|    1393318|[[SECADOR TAIFF, ...|         7|\n",
      "|    2056361|[[AUDIFONOS BASS,...|         8|\n",
      "|    1916890|[[1142550, 2019-1...|         9|\n",
      "|     392483|[[HARMAN KARDON, ...|        10|\n",
      "|     107670|[[2083291, 2019-1...|        11|\n",
      "|     537473|[[ORGANIZADOR SAP...|        12|\n",
      "|     673499|[[307861, 2019-09...|        13|\n",
      "|      65317|[[773066, 2019-10...|        14|\n",
      "|    1371364|[[FILHOTE CACHORR...|        15|\n",
      "|    1877560|[[876310, 2019-09...|        16|\n",
      "|        232|[[JBL GO 2, 2019-...|        17|\n",
      "|    1228971|[[REMEDIOS EMAGRE...|        18|\n",
      "|    1349384|[[744828, 2019-10...|        19|\n",
      "+-----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+--------------------+------------------+--------------------+----------+\n",
      "|item_bought|        user_history|session_id|               event|        event_info|     event_timestamp|event_type|\n",
      "+-----------+--------------------+----------+--------------------+------------------+--------------------+----------+\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1786148, 2019-10...|           1786148|2019-10-19T11:25:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1786148, 2019-10...|           1786148|2019-10-19T11:25:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[RELOGIO SMARTWAT...|RELOGIO SMARTWATCH|2019-10-19T11:26:...|    search|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T11:27:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T11:28:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T11:28:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T11:30:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T21:51:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T21:52:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T21:52:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T21:54:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T21:54:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T21:54:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-19T22:00:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-20T10:36:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-20T10:37:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-20T10:37:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-20T19:28:...|      view|\n",
      "|    1748830|[[1786148, 2019-1...|         0|[1615991, 2019-10...|           1615991|2019-10-20T19:28:...|      view|\n",
      "|     228737|[[643652, 2019-10...|         1|[643652, 2019-10-...|            643652|2019-10-06T18:02:...|      view|\n",
      "+-----------+--------------------+----------+--------------------+------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"event\", explode(df.user_history))\n",
    "\n",
    "df = df.withColumn('event_info', col(\"event\").getItem(\"event_info\"))\\\n",
    "        .withColumn('event_timestamp', col(\"event\").getItem(\"event_timestamp\"))\\\n",
    "        .withColumn('event_type', col(\"event\").getItem(\"event_type\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+----------+\n",
      "|session_id|     event_timestamp|        event_info|event_type|\n",
      "+----------+--------------------+------------------+----------+\n",
      "|         0|2019-10-19T11:25:...|           1786148|      view|\n",
      "|         0|2019-10-19T11:25:...|           1786148|      view|\n",
      "|         0|2019-10-19T11:26:...|RELOGIO SMARTWATCH|    search|\n",
      "|         0|2019-10-19T11:27:...|           1615991|      view|\n",
      "|         0|2019-10-19T11:28:...|           1615991|      view|\n",
      "|         0|2019-10-19T11:28:...|           1615991|      view|\n",
      "|         0|2019-10-19T11:30:...|           1615991|      view|\n",
      "|         0|2019-10-19T21:51:...|           1615991|      view|\n",
      "|         0|2019-10-19T21:52:...|           1615991|      view|\n",
      "|         0|2019-10-19T21:52:...|           1615991|      view|\n",
      "|         0|2019-10-19T21:54:...|           1615991|      view|\n",
      "|         0|2019-10-19T21:54:...|           1615991|      view|\n",
      "|         0|2019-10-19T21:54:...|           1615991|      view|\n",
      "|         0|2019-10-19T22:00:...|           1615991|      view|\n",
      "|         0|2019-10-20T10:36:...|           1615991|      view|\n",
      "|         0|2019-10-20T10:37:...|           1615991|      view|\n",
      "|         0|2019-10-20T10:37:...|           1615991|      view|\n",
      "|         0|2019-10-20T19:28:...|           1615991|      view|\n",
      "|         0|2019-10-20T19:28:...|           1615991|      view|\n",
      "|         1|2019-10-06T18:02:...|            643652|      view|\n",
      "+----------+--------------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df = df.select(\"session_id\", \"event_timestamp\", \"event_info\", \"event_type\")\n",
    "_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------+----------+\n",
      "|session_id|event_timestamp|event_info|event_type|\n",
      "+----------+---------------+----------+----------+\n",
      "|        26|     2019-10-26|   1567189|       buy|\n",
      "|        29|     2019-10-18|   1335130|       buy|\n",
      "|       474|     2019-10-10|   1495122|       buy|\n",
      "|       964|     2019-10-09|    892343|       buy|\n",
      "|      1677|     2019-10-24|    815397|       buy|\n",
      "|      1697|     2019-10-26|   1846850|       buy|\n",
      "|      1806|     2019-10-28|    223369|       buy|\n",
      "|      1950|     2019-09-28|    213136|       buy|\n",
      "|      2040|     2019-10-04|    394243|       buy|\n",
      "|      2214|     2019-10-12|    673207|       buy|\n",
      "|      2250|     2019-10-23|   1554497|       buy|\n",
      "|      2453|     2019-10-08|   1880554|       buy|\n",
      "|      2509|     2019-10-30|    985132|       buy|\n",
      "|      2529|     2019-10-05|   1867053|       buy|\n",
      "|      2927|     2019-10-08|    119967|       buy|\n",
      "|      3091|     2019-10-25|   1476214|       buy|\n",
      "|      3506|     2019-10-19|   2095502|       buy|\n",
      "|      3764|     2019-10-20|   1787803|       buy|\n",
      "|      4590|     2019-10-21|   1631735|       buy|\n",
      "|      4823|     2019-10-24|     75538|       buy|\n",
      "+----------+---------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as psf\n",
    "\n",
    "_df2 = df.groupBy(\"session_id\").agg(max(df.event_timestamp).alias(\"event_timestamp\"), \n",
    "                                    max(df.item_bought).alias(\"event_info\"))\n",
    "_df2 = _df2.withColumn('event_type', lit(\"buy\"))\n",
    "_df2 = _df2.withColumn('event_timestamp', F.date_add(_df2['event_timestamp'], 1))\n",
    "\n",
    "_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+----------+\n",
      "|session_id|     event_timestamp|event_info|event_type|\n",
      "+----------+--------------------+----------+----------+\n",
      "|         0|2019-10-19T11:25:...|   1786148|      view|\n",
      "|         0|2019-10-19T11:25:...|   1786148|      view|\n",
      "+----------+--------------------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = _df.union(_df2)\n",
    "dataset.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+----------+\n",
      "|session_id|     event_timestamp|          event_info|event_type|\n",
      "+----------+--------------------+--------------------+----------+\n",
      "|      4310|2019-10-25T07:19:...|  PROGRESSIVA FORMOL|    search|\n",
      "|      4310|2019-10-25T07:19:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-25T07:20:...|              617673|      view|\n",
      "|      4310|2019-10-25T07:20:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-25T07:20:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-25T07:20:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-25T07:20:...|  PROGRESSIVA FORMOL|    search|\n",
      "|      4310|2019-10-25T09:00:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:00:...|              756696|      view|\n",
      "|      4310|2019-10-25T09:00:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:00:...|              397019|      view|\n",
      "|      4310|2019-10-25T09:00:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:00:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:00:...|              142981|      view|\n",
      "|      4310|2019-10-25T09:00:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:01:...|              589573|      view|\n",
      "|      4310|2019-10-25T09:01:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:01:...|             1492506|      view|\n",
      "|      4310|2019-10-25T09:01:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:01:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:01:...|             1740417|      view|\n",
      "|      4310|2019-10-25T09:02:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:02:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:02:...|             1796722|      view|\n",
      "|      4310|2019-10-25T09:04:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:04:...|             2019999|      view|\n",
      "|      4310|2019-10-25T09:04:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:04:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:05:...|              397019|      view|\n",
      "|      4310|2019-10-25T09:05:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:05:...|              185471|      view|\n",
      "|      4310|2019-10-25T09:06:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:06:...|              793291|      view|\n",
      "|      4310|2019-10-25T09:06:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:06:...|             1962918|      view|\n",
      "|      4310|2019-10-25T09:06:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:06:...|             1010174|      view|\n",
      "|      4310|2019-10-25T09:07:...|             1010174|      view|\n",
      "|      4310|2019-10-25T09:07:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:07:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:07:...|             1702338|      view|\n",
      "|      4310|2019-10-25T09:07:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:07:...|              589573|      view|\n",
      "|      4310|2019-10-25T09:07:...|              589573|      view|\n",
      "|      4310|2019-10-25T09:07:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:08:...|              950398|      view|\n",
      "|      4310|2019-10-25T09:08:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:08:...|              840508|      view|\n",
      "|      4310|2019-10-25T09:08:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:08:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:08:...|             1700566|      view|\n",
      "|      4310|2019-10-25T09:08:...|             1700566|      view|\n",
      "|      4310|2019-10-25T09:08:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:09:...|              641353|      view|\n",
      "|      4310|2019-10-25T09:09:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:09:...|              758489|      view|\n",
      "|      4310|2019-10-25T09:09:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:09:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-25T09:15:...|   MARIA ESCANDALOSA|    search|\n",
      "|      4310|2019-10-26T18:45:...|              758489|      view|\n",
      "|      4310|2019-10-26T18:46:...|             1700566|      view|\n",
      "|      4310|2019-10-26T20:48:...|             REDMI 8|    search|\n",
      "|      4310|2019-10-26T20:49:...|             REDMI 8|    search|\n",
      "|      4310|2019-10-27T08:56:...|         SMART TV 32|    search|\n",
      "|      4310|2019-10-27T08:56:...|             1716388|      view|\n",
      "|      4310|2019-10-27T08:56:...|         SMART TV 32|    search|\n",
      "|      4310|2019-10-27T08:57:...|             1548617|      view|\n",
      "|      4310|2019-10-27T08:57:...|         SMART TV 32|    search|\n",
      "|      4310|2019-10-27T08:57:...|         SMART TV 32|    search|\n",
      "|      4310|2019-10-27T08:58:...|         SMART TV 32|    search|\n",
      "|      4310|2019-10-27T18:00:...|        BOTOX HANOVA|    search|\n",
      "|      4310|2019-10-27T18:00:...|        BOTOX HANOVA|    search|\n",
      "|      4310|2019-10-27T18:01:...|        BOTOX HANOVA|    search|\n",
      "|      4310|2019-10-27T18:01:...|             1793190|      view|\n",
      "|      4310|2019-10-27T18:01:...|        BOTOX HANOVA|    search|\n",
      "|      4310|2019-10-27T18:01:...|        BOTOX HANOVA|    search|\n",
      "|      4310|2019-10-27T18:01:...|               BOTOX|    search|\n",
      "|      4310|2019-10-27T18:01:...|               BOTOX|    search|\n",
      "|      4310|2019-10-27T18:02:...|               BOTOX|    search|\n",
      "|      4310|2019-10-27T18:02:...|        BOTOX HANOVA|    search|\n",
      "|      4310|2019-10-27T18:03:...|              269398|      view|\n",
      "|      4310|2019-10-27T18:05:...|              269398|      view|\n",
      "|      4310|2019-10-27T18:05:...|              269398|      view|\n",
      "|      4310|2019-10-28T08:09:...|              902962|      view|\n",
      "|      4310|2019-10-28T18:02:...|             2044396|      view|\n",
      "|      4310|2019-10-28T18:03:...|               98853|      view|\n",
      "|      4310|2019-10-28T18:03:...|             2044396|      view|\n",
      "|      4310|2019-10-28T18:03:...|             2040501|      view|\n",
      "|      4310|2019-10-28T18:03:...|             2044396|      view|\n",
      "|      4310|2019-10-28T19:51:...|             2040501|      view|\n",
      "|      4310|2019-10-28T19:51:...|             REDMI 8|    search|\n",
      "|      4310|2019-10-28T19:52:...|             1140939|      view|\n",
      "|      4310|2019-10-28T19:52:...|             REDMI 8|    search|\n",
      "|      4310|2019-10-28T19:52:...|             1097355|      view|\n",
      "|      4310|2019-10-28T19:53:...|             REDMI 8|    search|\n",
      "|      4310|2019-10-28T19:53:...|             REDMI 8|    search|\n",
      "|      4310|2019-10-28T19:54:...|        REDMI NOTE 7|    search|\n",
      "|      4310|2019-10-28T19:54:...|XIAOMI REDMI NOTE...|    search|\n",
      "|      4310|2019-10-28T19:54:...|XIAOMI REDMI NOTE...|    search|\n",
      "|      4310|2019-10-28T19:54:...|        REDMI NOTE 7|    search|\n",
      "|      4310|2019-10-28T19:55:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T19:56:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T19:56:...|              200488|      view|\n",
      "|      4310|2019-10-28T19:56:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T19:57:...|             2020516|      view|\n",
      "|      4310|2019-10-28T19:58:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T19:58:...|              971406|      view|\n",
      "|      4310|2019-10-28T19:58:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T19:59:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T19:59:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T19:59:...|             1864883|      view|\n",
      "|      4310|2019-10-28T20:01:...|             1864883|      view|\n",
      "|      4310|2019-10-28T20:01:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:02:...|REDMI NOTE 8 64GB...|    search|\n",
      "|      4310|2019-10-28T20:02:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:02:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:02:...|              495012|      view|\n",
      "|      4310|2019-10-28T20:03:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:04:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:04:...|              907403|      view|\n",
      "|      4310|2019-10-28T20:04:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:05:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:06:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:06:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:06:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:07:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:07:...|        REDMI NOTE 8|    search|\n",
      "|      4310|2019-10-28T20:07:...|        REDMI NOTE 8|    search|\n",
      "|      4310|2019-10-28T20:07:...|        REDMI NOTE 8|    search|\n",
      "|      4310|2019-10-28T20:07:...|        REDMI NOTE 8|    search|\n",
      "|      4310|2019-10-28T20:07:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:07:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:08:...|REDMI NOTE 8 64GB...|    search|\n",
      "|      4310|2019-10-28T20:08:...|REDMI NOTE 8 64GB...|    search|\n",
      "|      4310|2019-10-28T20:08:...|   REDMI NOTE 8 64GB|    search|\n",
      "|      4310|2019-10-28T20:08:...|        REDMI NOTE 7|    search|\n",
      "|      4310|2019-10-28T20:08:...|             REDMI 8|    search|\n",
      "|      4310|2019-10-29T11:45:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T11:45:...|     PROGRESSIVA CAP|    search|\n",
      "|      4310|2019-10-29T11:46:...|     PROGRESSIVA CAP|    search|\n",
      "|      4310|2019-10-29T11:46:...|PROGRESSIVA PROFI...|    search|\n",
      "|      4310|2019-10-29T11:46:...|PROGRESSIVA PROFI...|    search|\n",
      "|      4310|2019-10-29T11:47:...|PROGRESSIVA PROFI...|    search|\n",
      "|      4310|2019-10-29T11:47:...|              397019|      view|\n",
      "|      4310|2019-10-29T11:47:...|PROGRESSIVA PROFI...|    search|\n",
      "|      4310|2019-10-29T11:47:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T11:47:...|     PROGRESSIVA ZAP|    search|\n",
      "|      4310|2019-10-29T11:48:...|             1743534|      view|\n",
      "|      4310|2019-10-29T11:49:...|     PROGRESSIVA ZAP|    search|\n",
      "|      4310|2019-10-29T11:57:...|     PROGRESSIVA ZAP|    search|\n",
      "|      4310|2019-10-29T11:58:...|             1224582|      view|\n",
      "|      4310|2019-10-29T11:58:...|     PROGRESSIVA ZAP|    search|\n",
      "|      4310|2019-10-29T11:58:...|PROGRESSIVA JAPONESA|    search|\n",
      "|      4310|2019-10-29T11:58:...|             1425708|      view|\n",
      "|      4310|2019-10-29T11:59:...|             1822647|      view|\n",
      "|      4310|2019-10-29T11:59:...|             1425708|      view|\n",
      "|      4310|2019-10-29T11:59:...|PROGRESSIVA JAPONESA|    search|\n",
      "|      4310|2019-10-29T11:59:...|PROGRESSIVA JAPONESA|    search|\n",
      "|      4310|2019-10-29T11:59:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T11:59:...|             1290116|      view|\n",
      "|      4310|2019-10-29T12:00:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:00:...|              185471|      view|\n",
      "|      4310|2019-10-29T12:00:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:00:...|             2054852|      view|\n",
      "|      4310|2019-10-29T12:00:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:00:...|             2054852|      view|\n",
      "|      4310|2019-10-29T12:00:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:01:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:01:...|              524944|      view|\n",
      "|      4310|2019-10-29T12:01:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:01:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:02:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:02:...|              852053|      view|\n",
      "|      4310|2019-10-29T12:03:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:03:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:03:...|             1274135|      view|\n",
      "|      4310|2019-10-29T12:03:...|         PROGRESSIVA|    search|\n",
      "|      4310|2019-10-29T12:03:...|             1274135|      view|\n",
      "|      4310|2019-10-29T12:03:...|              617673|      view|\n",
      "|      4310|2019-10-29T12:04:...|              617673|      view|\n",
      "|      4310|2019-10-29T12:04:...|              852053|      view|\n",
      "|      4310|2019-10-29T12:04:...|              852053|      view|\n",
      "|      4310|2019-10-29T12:05:...|              617673|      view|\n",
      "|      4310|2019-10-29T12:06:...|              617673|      view|\n",
      "|      4310|2019-10-29T12:07:...|ESCOVA MARROQUINA...|    search|\n",
      "|      4310|2019-10-29T12:08:...|ESCOVA MARROQUINA...|    search|\n",
      "|      4310|2019-10-29T12:08:...|             1274135|      view|\n",
      "|      4310|2019-10-29T12:08:...|ESCOVA MARROQUINA...|    search|\n",
      "|      4310|2019-10-29T12:08:...|ESCOVA MARROQUINA...|    search|\n",
      "|      4310|2019-10-29T12:08:...|              979063|      view|\n",
      "|      4310|2019-10-29T12:08:...|ESCOVA MARROQUINA...|    search|\n",
      "|      4310|2019-10-29T12:08:...|              233512|      view|\n",
      "|      4310|2019-10-29T12:08:...|ESCOVA MARROQUINA...|    search|\n",
      "|      4310|2019-10-29T12:09:...|     BOTOX BLINDAGEM|    search|\n",
      "|      4310|2019-10-29T12:09:...|              421177|      view|\n",
      "|      4310|2019-10-29T12:10:...|              421177|      view|\n",
      "|      4310|2019-10-29T12:10:...|     BOTOX BLINDAGEM|    search|\n",
      "|      4310|2019-10-29T21:56:...|                 K11|    search|\n",
      "|      4310|2019-10-29T21:56:...|              165794|      view|\n",
      "|      4310|2019-10-29T21:56:...|                 K11|    search|\n",
      "|      4310|2019-10-29T21:57:...|              165794|      view|\n",
      "|      4310|2019-10-29T21:57:...|                 K11|    search|\n",
      "|      4310|2019-10-29T21:57:...|             1681830|      view|\n",
      "|      4310|2019-10-29T21:58:...|             1681830|      view|\n",
      "|      4310|2019-10-29T21:58:...|                 K11|    search|\n",
      "|      4310|2019-10-29T21:58:...|              549276|      view|\n",
      "|      4310|2019-10-29T21:58:...|                 K11|    search|\n",
      "|      4310|2019-10-29T21:58:...|             1610179|      view|\n",
      "|      4310|2019-10-29T21:59:...|             1610179|      view|\n",
      "|      4310|2019-10-29T21:59:...|                 K11|    search|\n",
      "|      4310|2019-10-29T22:00:...|                 K11|    search|\n",
      "|      4310|2019-10-29T22:00:...|                 K12|    search|\n",
      "|      4310|2019-10-29T22:00:...|             1650026|      view|\n",
      "|      4310|2019-10-29T22:01:...|                 K12|    search|\n",
      "|      4310|2019-10-29T22:01:...|                 K12|    search|\n",
      "|      4310|2019-10-29T22:02:...|                 K12|    search|\n",
      "|      4310|2019-10-29T22:02:...|                 K12|    search|\n",
      "|      4310|2019-10-30T17:18:...|                 K10|    search|\n",
      "|      4310|2019-10-30T17:18:...|             1778654|      view|\n",
      "|      4310|2019-10-30T17:18:...|                 K10|    search|\n",
      "|      4310|2019-10-30T17:18:...|             1274081|      view|\n",
      "|      4310|2019-10-30T17:18:...|                 K10|    search|\n",
      "|      4310|2019-10-30T17:19:...|         CELULAR K10|    search|\n",
      "|      4310|2019-10-30T17:19:...|         CELULAR K10|    search|\n",
      "|      4310|2019-10-30T17:19:...|    CELULAR K11 PLUS|    search|\n",
      "|      4310|2019-10-30T17:19:...|             1681830|      view|\n",
      "|      4310|2019-10-30T17:21:...|             1681830|      view|\n",
      "|      4310|2019-10-30T17:21:...|    CELULAR K11 PLUS|    search|\n",
      "|      4310|2019-10-30T17:21:...|              165794|      view|\n",
      "|      4310|2019-10-30T17:22:...|    CELULAR K11 PLUS|    search|\n",
      "|      4310|2019-10-30T17:22:...|         CELULAR K10|    search|\n",
      "|      4310|2019-10-30T17:58:...|         CELULAR K10|    search|\n",
      "|      4310|2019-10-30T17:58:...|         CELULAR K10|    search|\n",
      "|      4310|2019-10-30T17:59:...|         CELULAR K10|    search|\n",
      "|      4310|2019-10-30T17:59:...|         CELULAR K10|    search|\n",
      "|      4310|2019-10-30T17:59:...|         BOTOX BOTOX|    search|\n",
      "|      4310|2019-10-30T17:59:...|         CELULAR K10|    search|\n",
      "|      4310|2019-10-30T17:59:...|                 K10|    search|\n",
      "|      4310|2019-10-31T07:26:...|              165794|      view|\n",
      "|      4310|2019-10-31T07:26:...|             1385337|      view|\n",
      "|      4310|2019-10-31T11:26:...|              LG K11|    search|\n",
      "|      4310|2019-10-31T11:26:...|             1681830|      view|\n",
      "|      4310|2019-10-31T11:27:...|              LG K11|    search|\n",
      "|      4310|2019-10-31T11:27:...|             1855148|      view|\n",
      "|      4310|2019-10-31T11:28:...|             1855148|      view|\n",
      "|      4310|2019-10-31T11:28:...|             1855148|      view|\n",
      "|      4310|2019-10-31T11:28:...|              LG K11|    search|\n",
      "|      4310|2019-10-31T11:28:...|             1855148|      view|\n",
      "|      4310|2019-10-31T11:28:...|             1855148|      view|\n",
      "|      4310|2019-10-31T11:34:...|             1855148|      view|\n",
      "|      4310|2019-10-31T11:34:...|              LG K12|    search|\n",
      "|      4310|2019-10-31T11:35:...|             1650026|      view|\n",
      "|      4310|2019-10-31T11:36:...|              LG K12|    search|\n",
      "|      4310|2019-10-31T11:36:...|              LG K12|    search|\n",
      "|      4310|2019-10-31T11:36:...|              LG K11|    search|\n",
      "|      4310|2019-10-31T11:36:...|             1855148|      view|\n",
      "|      4310|2019-10-31T11:37:...|             1855148|      view|\n",
      "|      4310|2019-10-31T13:33:...|             1855148|      view|\n",
      "|      4310|2019-10-31T13:33:...|              LG K11|    search|\n",
      "|      4310|2019-10-31T13:33:...|                 TCL|    search|\n",
      "|      4310|2019-10-31T13:33:...|        TCL TV SMART|    search|\n",
      "|      4310|2019-10-31T13:34:...|              438477|      view|\n",
      "|      4310|2019-10-31T13:35:...|             1119507|      view|\n",
      "|      4310|2019-10-31T13:39:...|              438477|      view|\n",
      "|      4310|2019-10-31T13:39:...|        TCL TV SMART|    search|\n",
      "|      4310|2019-10-31T13:41:...|              551251|      view|\n",
      "|      4310|2019-10-31T13:41:...|        TCL TV SMART|    search|\n",
      "|      4310|2019-10-31T13:41:...|                 TCL|    search|\n",
      "|      4310|2019-10-31T13:41:...|              LG K11|    search|\n",
      "|      4310|2019-10-31T13:41:...|              LG K12|    search|\n",
      "|      4310|2019-10-31T13:41:...|             1855148|      view|\n",
      "|      4310|2019-10-31T13:41:...|              LG K11|    search|\n",
      "|      4310|          2019-11-01|             1855148|       buy|\n",
      "+----------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.filter(dataset.session_id == 4310).orderBy(col(\"event_timestamp\")).show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+----------+\n",
      "|session_id|     event_timestamp|event_info|event_type|\n",
      "+----------+--------------------+----------+----------+\n",
      "|         0|2019-10-19T11:25:...|   1786148|      view|\n",
      "|         0|2019-10-19T11:25:...|   1786148|      view|\n",
      "|         0|2019-10-19T11:27:...|   1615991|      view|\n",
      "|         0|2019-10-19T11:28:...|   1615991|      view|\n",
      "|         0|2019-10-19T11:28:...|   1615991|      view|\n",
      "|         0|2019-10-19T11:30:...|   1615991|      view|\n",
      "|         0|2019-10-19T21:51:...|   1615991|      view|\n",
      "|         0|2019-10-19T21:52:...|   1615991|      view|\n",
      "|         0|2019-10-19T21:52:...|   1615991|      view|\n",
      "|         0|2019-10-19T21:54:...|   1615991|      view|\n",
      "|         0|2019-10-19T21:54:...|   1615991|      view|\n",
      "|         0|2019-10-19T21:54:...|   1615991|      view|\n",
      "|         0|2019-10-19T22:00:...|   1615991|      view|\n",
      "|         0|2019-10-20T10:36:...|   1615991|      view|\n",
      "|         0|2019-10-20T10:37:...|   1615991|      view|\n",
      "|         0|2019-10-20T10:37:...|   1615991|      view|\n",
      "|         0|2019-10-20T19:28:...|   1615991|      view|\n",
      "|         0|2019-10-20T19:28:...|   1615991|      view|\n",
      "|         1|2019-10-06T18:02:...|    643652|      view|\n",
      "|         1|2019-10-07T09:46:...|   1156086|      view|\n",
      "+----------+--------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.filter(dataset.event_type != \"search\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, size\n",
    "\n",
    "df = spark.read.option(\"delimiter\", \",\").csv(\"/media/workspace/triplet_session/output/mercado_livre/dataset/session_dataset.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumnRenamed(\"session_id\", \"SessionID\")\\\n",
    "    .withColumnRenamed(\"event_timestamp2\", \"Timestamp\")\\\n",
    "    .withColumnRenamed(\"event_info\", \"ItemID\")\\\n",
    "    .withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\\\n",
    "    .orderBy(col('Timestamp'), col('SessionID')).select(\"SessionID\", \"ItemID\", \"Timestamp\", \"event_type\")\n",
    "\n",
    "# Remove Search event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12412331"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.event_type != \"search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+----------+\n",
      "|  SessionID|ItemID|           Timestamp|event_type|\n",
      "+-----------+------+--------------------+----------+\n",
      "|51539634491|873326|2019-09-24 08:42:...|      view|\n",
      "|77309438469|527037|2019-09-24 08:43:...|      view|\n",
      "+-----------+------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "df2 = df.limit(10000).cache()\n",
    "\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0210d98320d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "df2.schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+----------+\n",
      "|SessionID| ItemID|Timestamp|event_type|Timestamp2|\n",
      "+---------+-------+---------+----------+----------+\n",
      "|        0|1786148|     null|      view|      null|\n",
      "|        0|1786148|     null|      view|      null|\n",
      "+---------+-------+---------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2019-10-03T00:00:00.462-0400\n",
    "df2.withColumn(\"Timestamp2\", from_unixtime(unix_timestamp('Timestamp', 'yyyy-MM-dd HH:mm:ss.sssz'))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 10, 3, 0, 0, 0, 462000, tzinfo=tzoffset(None, -14400))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dateutil.parser import parse\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "\n",
    "parse(\"2019-10-03T00:00:00.462-0400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o85.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 244, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 74, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-809c23f56f92>\", line 1, in <lambda>\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 1358, in parse\n    return DEFAULTPARSER.parse(timestr, **kwargs)\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 646, in parse\n    res, skipped_tokens = self._parse(timestr, **kwargs)\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 722, in _parse\n    l = _timelex.split(timestr)         # Splits the timestr into tokens\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 207, in split\n    return list(cls(s))\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 76, in __init__\n    '{itype}'.format(itype=instream.__class__.__name__))\nTypeError: Parser must be a string or character stream, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:332)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:286)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 74, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-809c23f56f92>\", line 1, in <lambda>\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 1358, in parse\n    return DEFAULTPARSER.parse(timestr, **kwargs)\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 646, in parse\n    res, skipped_tokens = self._parse(timestr, **kwargs)\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 722, in _parse\n    l = _timelex.split(timestr)         # Splits the timestr into tokens\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 207, in split\n    return list(cls(s))\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 76, in __init__\n    '{itype}'.format(itype=instream.__class__.__name__))\nTypeError: Parser must be a string or character stream, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:332)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:286)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-809c23f56f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparse_date\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mudf\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Timestamp2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.3.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o85.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 244, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 74, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-809c23f56f92>\", line 1, in <lambda>\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 1358, in parse\n    return DEFAULTPARSER.parse(timestr, **kwargs)\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 646, in parse\n    res, skipped_tokens = self._parse(timestr, **kwargs)\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 722, in _parse\n    l = _timelex.split(timestr)         # Splits the timestr into tokens\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 207, in split\n    return list(cls(s))\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 76, in __init__\n    '{itype}'.format(itype=instream.__class__.__name__))\nTypeError: Parser must be a string or character stream, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:332)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:286)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 74, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/opt/spark-2.3.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-809c23f56f92>\", line 1, in <lambda>\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 1358, in parse\n    return DEFAULTPARSER.parse(timestr, **kwargs)\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 646, in parse\n    res, skipped_tokens = self._parse(timestr, **kwargs)\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 722, in _parse\n    l = _timelex.split(timestr)         # Splits the timestr into tokens\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 207, in split\n    return list(cls(s))\n  File \"/home/marlesson/.local/lib/python3.7/site-packages/dateutil/parser/_parser.py\", line 76, in __init__\n    '{itype}'.format(itype=instream.__class__.__name__))\nTypeError: Parser must be a string or character stream, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:332)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:286)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "parse_date =  udf (lambda x: parse(x), TimestampType())\n",
    "\n",
    "df2.withColumn('Timestamp2', parse_date(col('Timestamp'))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates(['SessionID', 'ItemID', 'event_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'datetime.timedelta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9afc60e33233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max(Timestamp)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minit_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_timestamp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'datetime.timedelta'"
     ]
    }
   ],
   "source": [
    "max_timestamp = df.select(max(col('Timestamp'))).collect()[0]['max(Timestamp)']\n",
    "init_timestamp = max_timestamp - timedelta(days = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_timestamp, max_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-with-Pixiedust_Spark-2.3",
   "language": "python",
   "name": "pythonwithpixiedustspark23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
